{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tokenizers\n",
      "  Using cached tokenizers-0.19.1-cp310-none-win_amd64.whl (2.2 MB)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4\n",
      "  Using cached huggingface_hub-0.24.5-py3-none-any.whl (417 kB)\n",
      "Collecting pyyaml>=5.1\n",
      "  Using cached PyYAML-6.0.1-cp310-cp310-win_amd64.whl (145 kB)\n",
      "Collecting requests\n",
      "  Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Using cached fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sulai\\documents\\gh-projects\\tokenization\\env\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.12.2)\n",
      "Collecting tqdm>=4.42.1\n",
      "  Using cached tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\sulai\\documents\\gh-projects\\tokenization\\env\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.1)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.15.4-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\sulai\\documents\\gh-projects\\tokenization\\env\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub<1.0,>=0.16.4->tokenizers) (0.4.6)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Using cached charset_normalizer-3.3.2-cp310-cp310-win_amd64.whl (100 kB)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Using cached urllib3-2.2.2-py3-none-any.whl (121 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.7-py3-none-any.whl (66 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2024.7.4-py3-none-any.whl (162 kB)\n",
      "Installing collected packages: urllib3, tqdm, pyyaml, idna, fsspec, filelock, charset-normalizer, certifi, requests, huggingface-hub, tokenizers\n",
      "Successfully installed certifi-2024.7.4 charset-normalizer-3.3.2 filelock-3.15.4 fsspec-2024.6.1 huggingface-hub-0.24.5 idna-3.7 pyyaml-6.0.1 requests-2.32.3 tokenizers-0.19.1 tqdm-4.66.4 urllib3-2.2.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import NFD, StripAccents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello how are u?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer = normalizers.Sequence([NFD(), StripAccents()])\n",
    "normalizer.normalize_str(\"Héllò hôw are ü?\")\n",
    "\n",
    "# 'Hello how are u?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', (0, 5)),\n",
       " ('!', (5, 6)),\n",
       " ('How', (7, 10)),\n",
       " ('are', (11, 14)),\n",
       " ('you', (15, 18)),\n",
       " ('?', (18, 19)),\n",
       " ('I', (20, 21)),\n",
       " (\"'\", (21, 22)),\n",
       " ('m', (22, 23)),\n",
       " ('fine', (24, 28)),\n",
       " (',', (28, 29)),\n",
       " ('thank', (30, 35)),\n",
       " ('you', (36, 39)),\n",
       " ('.', (39, 40))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "pre_tokenizer = Whitespace()\n",
    "pre_tokenizer.pre_tokenize_str(\"Hello! How are you? I'm fine, thank you.\")\n",
    "\n",
    "\n",
    "# [('Hello', (0, 5)),\n",
    "# ('!', (5, 6)),\n",
    "# ('How', (7, 10)),\n",
    "# ('are', (11, 14)),\n",
    "# ('you', (15, 18)),\n",
    "# ('?', (18, 19)),\n",
    "# ('I', (20, 21)),\n",
    "# (\"'\", (21, 22)),\n",
    "# ('m', (22, 23)),\n",
    "# ('fine', (24, 28)),\n",
    "# (',', (28, 29)),\n",
    "# ('thank', (30, 35)),\n",
    "# ('you', (36, 39)),\n",
    "# ('.', (39, 40))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Call', (0, 4)), ('9', (5, 6)), ('1', (6, 7)), ('1', (7, 8)), ('!', (8, 9))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import pre_tokenizers\n",
    "from tokenizers.pre_tokenizers import Digits\n",
    "pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Digits(individual_digits=True)])\n",
    "pre_tokenizer.pre_tokenize_str(\"Call 911!\")\n",
    "\n",
    "# [('Call', (0, 4)), ('9', (5, 6)), ('1', (6, 7)), ('1', (7, 8)), ('!', (8, 9))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.processors import TemplateProcessing\n",
    "# import tokenizers\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "tokenizer = Tokenizer.from_file(\"Meta-Llama-3.1-8B-bnb-4bit/tokenizer.json\")\n",
    "\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[(\"[CLS]\", 1), (\"[SEP]\", 2)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "bert_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import NFD, Lowercase, StripAccents\n",
    "bert_tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "bert_tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.processors import TemplateProcessing\n",
    "bert_tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", 1),\n",
    "        (\"[SEP]\", 2),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.2.2-cp310-cp310-win_amd64.whl (11.6 MB)\n",
      "Collecting pyarrow\n",
      "  Using cached pyarrow-17.0.0-cp310-cp310-win_amd64.whl (25.1 MB)\n",
      "Collecting tzdata>=2022.7\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Collecting numpy>=1.22.4\n",
      "  Using cached numpy-2.0.1-cp310-cp310-win_amd64.whl (16.6 MB)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sulai\\documents\\gh-projects\\tokenization\\env\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sulai\\documents\\gh-projects\\tokenization\\env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Installing collected packages: pytz, tzdata, numpy, pyarrow, pandas\n",
      "Successfully installed numpy-2.0.1 pandas-2.2.2 pyarrow-17.0.0 pytz-2024.1 tzdata-2024.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Using cached datasets-2.20.0-py3-none-any.whl (547 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sulai\\documents\\gh-projects\\tokenization\\env\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\sulai\\documents\\gh-projects\\tokenization\\env\\lib\\site-packages (from datasets) (2.0.1)\n",
      "Collecting dill<0.3.9,>=0.3.0\n",
      "  Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\sulai\\documents\\gh-projects\\tokenization\\env\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Collecting fsspec[http]<=2024.5.0,>=2023.1.0\n",
      "  Using cached fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
      "Collecting xxhash\n",
      "  Using cached xxhash-3.4.1-cp310-cp310-win_amd64.whl (29 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\sulai\\documents\\gh-projects\\tokenization\\env\\lib\\site-packages (from datasets) (3.15.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\sulai\\documents\\gh-projects\\tokenization\\env\\lib\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\sulai\\documents\\gh-projects\\tokenization\\env\\lib\\site-packages (from datasets) (4.66.4)\n",
      "Collecting multiprocess\n",
      "  Using cached multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Collecting aiohttp\n",
      "  Using cached aiohttp-3.10.0-cp310-cp310-win_amd64.whl (375 kB)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\sulai\\documents\\gh-projects\\tokenization\\env\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in c:\\users\\sulai\\documents\\gh-projects\\tokenization\\env\\lib\\site-packages (from datasets) (0.24.5)\n",
      "Collecting pyarrow-hotfix\n",
      "  Using cached pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\sulai\\documents\\gh-projects\\tokenization\\env\\lib\\site-packages (from datasets) (17.0.0)\n",
      "Collecting attrs>=17.3.0\n",
      "  Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Using cached multidict-6.0.5-cp310-cp310-win_amd64.whl (28 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Using cached yarl-1.9.4-cp310-cp310-win_amd64.whl (76 kB)\n",
      "Collecting async-timeout<5.0,>=4.0\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0\n",
      "  Using cached aiohappyeyeballs-2.3.4-py3-none-any.whl (12 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Using cached frozenlist-1.4.1-cp310-cp310-win_amd64.whl (50 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sulai\\documents\\gh-projects\\tokenization\\env\\lib\\site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sulai\\documents\\gh-projects\\tokenization\\env\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sulai\\documents\\gh-projects\\tokenization\\env\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sulai\\documents\\gh-projects\\tokenization\\env\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sulai\\documents\\gh-projects\\tokenization\\env\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\sulai\\documents\\gh-projects\\tokenization\\env\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sulai\\documents\\gh-projects\\tokenization\\env\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sulai\\documents\\gh-projects\\tokenization\\env\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sulai\\documents\\gh-projects\\tokenization\\env\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sulai\\documents\\gh-projects\\tokenization\\env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: xxhash, pyarrow-hotfix, multidict, fsspec, frozenlist, dill, attrs, async-timeout, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.6.1\n",
      "    Uninstalling fsspec-2024.6.1:\n",
      "      Successfully uninstalled fsspec-2024.6.1\n",
      "Successfully installed aiohappyeyeballs-2.3.4 aiohttp-3.10.0 aiosignal-1.3.1 async-timeout-4.0.3 attrs-23.2.0 datasets-2.20.0 dill-0.3.8 frozenlist-1.4.1 fsspec-2024.5.0 multidict-6.0.5 multiprocess-0.70.16 pyarrow-hotfix-0.6 xxhash-3.4.1 yarl-1.9.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas pyarrow\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sulai\\Documents\\GH-Projects\\Tokenization\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"iohadrubin/wikitext-103-raw-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 29567\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 60\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 62\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer training completed and saved to data/bert-wiki.json\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "import os\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"iohadrubin/wikitext-103-raw-v1\")\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(\"data/wikitext-103-raw\", exist_ok=True)\n",
    "\n",
    "# Save the dataset splits to text files\n",
    "splits = [\"test\", \"train\", \"validation\"]\n",
    "files = []\n",
    "for split in splits:\n",
    "    text_file = f\"data/wikitext-103-raw/wiki.{split}.raw\"\n",
    "    with open(text_file, 'w', encoding='utf-8') as f:\n",
    "        for line in dataset[split]['text']:\n",
    "            f.write(line + '\\n')\n",
    "    files.append(text_file)\n",
    "\n",
    "# Initialize the BertWordPieceTokenizer\n",
    "bert_tokenizer = BertWordPieceTokenizer()\n",
    "\n",
    "# Train the tokenizer directly using parameters\n",
    "bert_tokenizer.train(\n",
    "    files,\n",
    "    vocab_size=30522,\n",
    "    special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    ")\n",
    "\n",
    "# Save the tokenizer\n",
    "bert_tokenizer.save(\"data/bert-wiki.json\")\n",
    "\n",
    "print(\"Tokenizer training completed and saved to data/bert-wiki.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 9906, 11, 379, 65948, 0, 2650, 527, 499, 27623, 223, 949, 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"yer1~, takes&(\\r\\n Then.bind!D#'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tokenizer.encode(\"Hello, y'all! How are you 😁 ?\")\n",
    "print(output.ids)\n",
    "# [1, 27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35, 2]\n",
    "tokenizer.decode([1, 27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35, 2])\n",
    "# \"Hello , y ' all ! How are you ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['welcome', 'to', 'the', '[UNK]', 'token', '##izers', 'library', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'welcome to the tokenizers library.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = bert_tokenizer.encode(\"Welcome to the 🤗 Tokenizers library.\")\n",
    "print(output.tokens)\n",
    "# [\"[CLS]\", \"welcome\", \"to\", \"the\", \"[UNK]\", \"tok\", \"##eni\", \"##zer\", \"##s\", \"library\", \".\", \"[SEP]\"]\n",
    "bert_tokenizer.decode(output.ids)\n",
    "# \"welcome to the tok ##eni ##zer ##s library .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'welcome to the tokenizers library.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import decoders\n",
    "bert_tokenizer.decoder = decoders.WordPiece()\n",
    "bert_tokenizer.decode(output.ids)\n",
    "# \"welcome to the tokenizers library.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
